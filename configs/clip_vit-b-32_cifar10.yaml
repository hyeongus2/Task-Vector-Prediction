model:
    name: vit-b-32
    pretrained: laion2b_s34b_b79k

data:
    name: CIFAR10
    output_dim: 10
    batch_size: 128

finetuning:
    method: lora
    epochs: 20
    lr: 1.e-3
    optimizer: sgd
    momentum: 0.0

save:
    enabled: true
    every: 30

logging:
    enabled: true
    use_wandb: true

analyze:
    modes: epoch
    indices: 5
    alpha_grid: [0.001, 0.01, 0.1, 0.5, 1]
